project:
  name: "linear-regression-housing"

paths:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  models_dir: "models"
  reports_dir: "reports"
  figures_dir: "reports/figures"

data:
  target_col: "trip_duration"
  log_target: true # Here Is True because the value are skewed, so we need to log transform the target value to reduce skewness.
  id: "id"
  test_size: 0.30
  random_state: 42


training:
  mode: "val"          # "cv" OR "val" cv = cross validation
  val_size: 0.2       # used only if mode="val" (split from train, not from full data)
  cv_folds: 5         # used only if mode="cv"


model:
  name: "model.joblib"
  type: "light_gbm"
  objective: "regression" # Cause it's a problem
  boosting_type: "gbdt" # boosting_type tells LightGBM how trees are combined during boosting.gbdt mean Gradient Boosted Decision Trees.gbdt train sequentially: Train a first tree → rough predictions, and then Compute errors (residuals / gradients), after that train next tree to correct those errors and Repeat 
  n_estimators: 800 # Number of trees in the forest(More trees reduce vaiance,improve stability, but too many trees will slower the training). Each tree is a small correction to the model, so n_estimators is how many correction steps you allow.
  learning_rate: 0.05 # Smaller steps → better generalization
  max_depth: 6 # This is the Maximum depth of each tree, high depth captures complex interactions and lower depth smoother generalization.
  num_leaves: 31 # Controls tree complexity, Works well with depth ≤ 6.
  min_child_samples: 40 # Prevents tiny noisy leaves, Crucial for imbalanced data
  subsample: 0.8 # Adds randomness, Equivalent to “bagging inside boosting”
  colsample_bytree: 0.8 # Adds randomness, Equivalent to “bagging inside boosting”
  n_jobs: -1


evaluation:
  save_figures: true

